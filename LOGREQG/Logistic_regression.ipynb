{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer les packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sweetviz as sv\n",
    "import missingno as msno\n",
    "import time \n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer, make_column_transformer\n",
    "# sklearn.compose: The sklearn.compose module is a submodule of the sklearn library for machine learning in Python. It provides functions for creating complex preprocessing and modeling pipelines.\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,PolynomialFeatures,RobustScaler\n",
    "#sklearn.preprocessing: The sklearn.preprocessing module is a submodule of the sklearn library that provides functions for preprocessing data, such as scaling and normalizing features, imputing missing values, and encoding categorical variables.\n",
    "from sklearn.linear_model import Ridge,LinearRegression,Lasso, ElasticNet\n",
    "# sklearn.linear_model: The sklearn.linear_model module is a submodule of the sklearn library that provides functions for fitting linear models for regression and classification.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# sklearn.pipeline: The sklearn.pipeline module is a submodule of the sklearn library that provides functions for creating and working with pipelines of transformers and models.\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,learning_curve, RandomizedSearchCV, cross_val_score, KFold\n",
    "# sklearn.model_selection: The sklearn.model_selection module is a submodule of the sklearn library that provides functions for splitting data into training and test sets, evaluating models using cross-validation, and hyperparameter tuning.\n",
    "from sklearn.dummy import DummyRegressor\n",
    "# sklearn.dummy: The sklearn.dummy module is a submodule of the sklearn library that provides simple dummy models for regression and classification.\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lire les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33020/2682208335.py:1: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(\"/home/apprenant/Documents/DATA/loan_project/SBAnational.csv\")\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"/home/apprenant/Documents/DATA/loan_project/SBAnational.csv\")\n",
    "\n",
    "df['ApprovalFY'] = df['ApprovalFY'].replace('A', '', regex=True).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33020/2495417656.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df[col] = df[col].str.replace(\"$\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# create a list of all numeric columns\n",
    "columns_to_transform_to_int = [\"DisbursementGross\",\"BalanceGross\",\"ChgOffPrinGr\",\"GrAppv\",\"SBA_Appv\"]\n",
    "\n",
    "\n",
    "for col in columns_to_transform_to_int:\n",
    "    df[col] = df[col].str.replace(\"$\", \"\")\n",
    "    df[col] = df[col].str.replace(\",\", \"\")\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "\n",
    "df['Term'] = df['Term'].astype(int)\n",
    "\n",
    "df['LowDoc'] = df['LowDoc'].replace({'0': 'N'})\n",
    "df = df[df['LowDoc'].isin(['N','Y'])]\n",
    "df['RevLineCr'] = df['RevLineCr'].replace({'0': 'N', 'T':'Y'})\n",
    "df = df[df['RevLineCr'].isin(['N','Y'])]\n",
    "\n",
    "dictionnaire =  {\"11\":\"Agriculture, forestry, fishing and hunting\",\"72\":\"Accommodation and food services\",\"21\":\"Mining, quarrying, and oil and gas extraction\",\"22\":\n",
    "\"Utilities\",\"23\":\"Construction\",\"31\":\"Manufacturing\",\"32\":\"Manufacturing\",\"33\":\"Manufacturing\",\"42\":\"Wholesale trade\",\"44\":\"Retail trade\",\"45\":\"Retail trade\",\"48\":\" Transportation and warehousing\", \"49\":\"Transportation and warehousing\", \"51\":\"Information\",\"52\":\"Finance and insurance\", \"53\":\"Real estate and rental and leasing\",\"54\":\"Professional, scientific, and technical services\",\"55\":\"Management of companies and enterprises\",\"56\":\"Administrative and support and waste management and remediation services\",\"61\":\"Educational services\",\"62\":\"Health care and social assistance\",\"71\":\"Arts, entertainment, and recreation\",\"81\":\"Other services (except public administration)\", \"92\": \"Public administration\", \"0\" : \"Other\"}\n",
    "\n",
    "df['NAICS'] = df['NAICS'].astype(str).str[:2]\n",
    "\n",
    "df[\"NAICS\"] = df[\"NAICS\"].map(dictionnaire)\n",
    "\n",
    "cols_to_drop = ['LoanNr_ChkDgt', 'Name', 'City', 'Zip', 'Bank', 'BankState', 'ApprovalDate', 'RetainedJob','ChgOffDate', 'DisbursementDate', 'DisbursementGross', 'BalanceGross', 'SBA_Appv', 'ChgOffPrinGr']\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "df_cleaned.dropna(subset=['MIS_Status'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation à la modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleaned.MIS_Status\n",
    "X = df_cleaned.drop(columns=['MIS_Status'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline_to_ML(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.8, random_state=42)\n",
    "    numerical_features = make_column_selector(dtype_include=np.number)\n",
    "    categorical_features = make_column_selector(dtype_exclude= np.number)\n",
    "    numerical_pipeline = make_pipeline(SimpleImputer(strategy='mean'),StandardScaler(with_mean=False))\n",
    "    categorical_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder(handle_unknown='ignore'))\n",
    "    preprocessor = make_column_transformer((numerical_pipeline, numerical_features),\n",
    "                                    (categorical_pipeline, categorical_features)\n",
    "                                    )\n",
    "    return preprocessor, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "preprocessor, X_train, X_test, y_train, y_test = make_pipeline_to_ML(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "![Logistic](https://miro.medium.com/max/1400/0*q3wC98LKNGFSFwdG.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# les paramètres \n",
    "- `C` : C est un coefficient de régularisation qui contrôle la force de la pénalité dans la régression logistique. Plus C est grand, plus la régularisation est faible, ce qui peut conduire à un modèle plus complexe et à un ajustement sur-adapté.\n",
    "- `penalty` : spécifie la forme de la pénalité pour la régularisation, 'l1' pour la pénalité L1 (régularisation Lasso) et 'l2' pour la pénalité L2 (régularisation Ridge).\n",
    "- `solver` : 'saga' est un algorithme de résolution utilisé pour la régression logistique.\n",
    "- `max_iter` : spécifie le nombre maximal d'itérations pour la convergence de l'algorithme.\n",
    "- `fit_intercept` : spécifie si le modèle doit inclure une constante (intercept) ou non.\n",
    "- `tol` : spécifie la tolérance pour la convergence de l'algorithme, c'est-à-dire l'erreur maximale admissible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline ... OK\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "Fit ... OK\n",
      "------------------------------------------------------------\n",
      "Predict ... OK\n",
      "Accuracy: 0.7151141513948709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.36      0.82      0.50     31281\n",
      "       P I F       0.95      0.69      0.80    146334\n",
      "\n",
      "    accuracy                           0.72    177615\n",
      "   macro avg       0.66      0.76      0.65    177615\n",
      "weighted avg       0.84      0.72      0.75    177615\n",
      "\n",
      "[[ 25701   5580]\n",
      " [ 45020 101314]]\n",
      "f1_score_average : Macro : 0.6520556851690158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Add LogisticRegression to the pipeline\n",
    "pipeline = make_pipeline(preprocessor, LogisticRegression(max_iter=10000, class_weight='balanced', n_jobs=-1))\n",
    "print(\"pipeline ... OK\")\n",
    "print(\"---\"*20)\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"---\"*20)\n",
    "print(\"Fit ... OK\")\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"---\"*20)\n",
    "print(\"Predict ... OK\")\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print('Accuracy:', score)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"f1_score_average : Macro :\",f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline ... OK\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "Fit ... OK\n",
      "------------------------------------------------------------\n",
      "Predict ... OK\n",
      "Accuracy: 0.7151141513948709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.36      0.82      0.50     31281\n",
      "       P I F       0.95      0.69      0.80    146334\n",
      "\n",
      "    accuracy                           0.72    177615\n",
      "   macro avg       0.66      0.76      0.65    177615\n",
      "weighted avg       0.84      0.72      0.75    177615\n",
      "\n",
      "[[ 25701   5580]\n",
      " [ 45020 101314]]\n",
      "f1_score_average : Macro : 0.6520556851690158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Add LogisticRegression to the pipeline\n",
    "pipeline = make_pipeline(preprocessor, LogisticRegression(C=1.0, penalty='l2', solver='lbfgs', max_iter=10000, class_weight='balanced', n_jobs=-1))\n",
    "\n",
    "print(\"pipeline ... OK\")\n",
    "print(\"---\"*20)\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"---\"*20)\n",
    "print(\"Fit ... OK\")\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"---\"*20)\n",
    "print(\"Predict ... OK\")\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print('Accuracy:', score)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"f1_score_average : Macro :\",f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarder pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open(\"LogisREG.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analyse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efb016019911489f5a03966f4d1632bb4674bdc6f12f4d95624e206dc03ec67a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
