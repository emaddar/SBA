{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sweetviz as sv\n",
    "import missingno as msno\n",
    "import time \n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer, make_column_transformer\n",
    "# sklearn.compose: The sklearn.compose module is a submodule of the sklearn library for machine learning in Python. It provides functions for creating complex preprocessing and modeling pipelines.\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,PolynomialFeatures,RobustScaler\n",
    "#sklearn.preprocessing: The sklearn.preprocessing module is a submodule of the sklearn library that provides functions for preprocessing data, such as scaling and normalizing features, imputing missing values, and encoding categorical variables.\n",
    "from sklearn.linear_model import Ridge,LinearRegression,Lasso, ElasticNet\n",
    "# sklearn.linear_model: The sklearn.linear_model module is a submodule of the sklearn library that provides functions for fitting linear models for regression and classification.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# sklearn.pipeline: The sklearn.pipeline module is a submodule of the sklearn library that provides functions for creating and working with pipelines of transformers and models.\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,learning_curve, RandomizedSearchCV, cross_val_score, KFold\n",
    "# sklearn.model_selection: The sklearn.model_selection module is a submodule of the sklearn library that provides functions for splitting data into training and test sets, evaluating models using cross-validation, and hyperparameter tuning.\n",
    "from sklearn.dummy import DummyRegressor\n",
    "# sklearn.dummy: The sklearn.dummy module is a submodule of the sklearn library that provides simple dummy models for regression and classification.\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38199/2682208335.py:1: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(\"/home/apprenant/Documents/DATA/loan_project/SBAnational.csv\")\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"/home/apprenant/Documents/DATA/loan_project/SBAnational.csv\")\n",
    "\n",
    "df['ApprovalFY'] = df['ApprovalFY'].replace('A', '', regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38199/2495417656.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df[col] = df[col].str.replace(\"$\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# create a list of all numeric columns\n",
    "columns_to_transform_to_int = [\"DisbursementGross\",\"BalanceGross\",\"ChgOffPrinGr\",\"GrAppv\",\"SBA_Appv\"]\n",
    "\n",
    "\n",
    "for col in columns_to_transform_to_int:\n",
    "    df[col] = df[col].str.replace(\"$\", \"\")\n",
    "    df[col] = df[col].str.replace(\",\", \"\")\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "\n",
    "df['Term'] = df['Term'].astype(int)\n",
    "\n",
    "df['LowDoc'] = df['LowDoc'].replace({'0': 'N'})\n",
    "df = df[df['LowDoc'].isin(['N','Y'])]\n",
    "df['RevLineCr'] = df['RevLineCr'].replace({'0': 'N', 'T':'Y'})\n",
    "df = df[df['RevLineCr'].isin(['N','Y'])]\n",
    "\n",
    "dictionnaire =  {\"11\":\"Agriculture, forestry, fishing and hunting\",\"72\":\"Accommodation and food services\",\"21\":\"Mining, quarrying, and oil and gas extraction\",\"22\":\n",
    "\"Utilities\",\"23\":\"Construction\",\"31\":\"Manufacturing\",\"32\":\"Manufacturing\",\"33\":\"Manufacturing\",\"42\":\"Wholesale trade\",\"44\":\"Retail trade\",\"45\":\"Retail trade\",\"48\":\" Transportation and warehousing\", \"49\":\"Transportation and warehousing\", \"51\":\"Information\",\"52\":\"Finance and insurance\", \"53\":\"Real estate and rental and leasing\",\"54\":\"Professional, scientific, and technical services\",\"55\":\"Management of companies and enterprises\",\"56\":\"Administrative and support and waste management and remediation services\",\"61\":\"Educational services\",\"62\":\"Health care and social assistance\",\"71\":\"Arts, entertainment, and recreation\",\"81\":\"Other services (except public administration)\", \"92\": \"Public administration\", \"0\" : \"Other\"}\n",
    "\n",
    "df['NAICS'] = df['NAICS'].astype(str).str[:2]\n",
    "\n",
    "df[\"NAICS\"] = df[\"NAICS\"].map(dictionnaire)\n",
    "\n",
    "cols_to_drop = ['LoanNr_ChkDgt', 'Name', 'City', 'Zip', 'Bank', 'BankState', 'ApprovalDate', 'RetainedJob','ChgOffDate', 'DisbursementDate', 'DisbursementGross', 'BalanceGross', 'SBA_Appv', 'ChgOffPrinGr']\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "df_cleaned.dropna(subset=['MIS_Status'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleaned.MIS_Status\n",
    "X = df_cleaned.drop(columns=['MIS_Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline_to_ML(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.8, random_state=42)\n",
    "    numerical_features = make_column_selector(dtype_include=np.number)\n",
    "    categorical_features = make_column_selector(dtype_exclude= np.number)\n",
    "    numerical_pipeline = make_pipeline(SimpleImputer(strategy='mean'),StandardScaler(with_mean=False))\n",
    "    categorical_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder(handle_unknown='ignore'))\n",
    "    preprocessor = make_column_transformer((numerical_pipeline, numerical_features),\n",
    "                                    (categorical_pipeline, categorical_features)\n",
    "                                    )\n",
    "    return preprocessor, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "preprocessor, X_train, X_test, y_train, y_test = make_pipeline_to_ML(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "![Logistic](https://miro.medium.com/max/1400/0*q3wC98LKNGFSFwdG.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline ... OK\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "Fit ... OK\n",
      "------------------------------------------------------------\n",
      "Predict ... OK\n",
      "Accuracy: 0.7151141513948709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.36      0.82      0.50     31281\n",
      "       P I F       0.95      0.69      0.80    146334\n",
      "\n",
      "    accuracy                           0.72    177615\n",
      "   macro avg       0.66      0.76      0.65    177615\n",
      "weighted avg       0.84      0.72      0.75    177615\n",
      "\n",
      "[[ 25701   5580]\n",
      " [ 45020 101314]]\n",
      "f1_score_average : Macro : 0.6520556851690158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Add LogisticRegression to the pipeline\n",
    "pipeline = make_pipeline(preprocessor, LogisticRegression(max_iter=10000, class_weight='balanced', n_jobs=-1))\n",
    "print(\"pipeline ... OK\")\n",
    "print(\"---\"*20)\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"---\"*20)\n",
    "print(\"Fit ... OK\")\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"---\"*20)\n",
    "print(\"Predict ... OK\")\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print('Accuracy:', score)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"f1_score_average : Macro :\",f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline ... OK\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "Fit ... OK\n",
      "------------------------------------------------------------\n",
      "Predict ... OK\n",
      "Accuracy: 0.7131379669509895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CHGOFF       0.36      0.83      0.50     31281\n",
      "       P I F       0.95      0.69      0.80    146334\n",
      "\n",
      "    accuracy                           0.71    177615\n",
      "   macro avg       0.66      0.76      0.65    177615\n",
      "weighted avg       0.85      0.71      0.75    177615\n",
      "\n",
      "[[ 25922   5359]\n",
      " [ 45592 100742]]\n",
      "f1_score_average : Macro : 0.6512527497712916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Add LogisticRegression to the pipeline\n",
    "pipeline = make_pipeline(preprocessor, LogisticRegression(C=0.001, penalty='l2',solver='lbfgs',max_iter=10000, class_weight='balanced', n_jobs=-1, fit_intercept=False,tol=0.0001))\n",
    "\n",
    "print(\"pipeline ... OK\")\n",
    "print(\"---\"*20)\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"---\"*20)\n",
    "print(\"Fit ... OK\")\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"---\"*20)\n",
    "print(\"Predict ... OK\")\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print('Accuracy:', score)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"f1_score_average : Macro :\",f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C : C est un coefficient de régularisation qui contrôle la force de la pénalité dans la régression logistique. Plus C est grand, plus la régularisation est faible, ce qui peut conduire à un modèle plus complexe et à un ajustement sur-adapté.\n",
    "\n",
    "penalty : spécifie la forme de la pénalité pour la régularisation, 'l1' pour la pénalité L1 (régularisation Lasso) et 'l2' pour la pénalité L2 (régularisation Ridge).\n",
    "\n",
    "solver : 'saga' est un algorithme de résolution utilisé pour la régression logistique.\n",
    "\n",
    "max_iter : spécifie le nombre maximal d'itérations pour la convergence de l'algorithme.\n",
    "\n",
    "fit_intercept : spécifie si le modèle doit inclure une constante (intercept) ou non.\n",
    "\n",
    "tol : spécifie la tolérance pour la convergence de l'algorithme, c'est-à-dire l'erreur maximale admissible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Define the hyperparameters to be tuned\n",
    "hyperparameters = {'logisticregression__C': [0.1],\n",
    "# 'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                #    'logisticregression__penalty': ['l1', 'l2'],\n",
    "                #    'logisticregression__fit_intercept': [True, False]\n",
    "                   }\n",
    "\n",
    "# Create the pipeline with the LogisticRegression model\n",
    "pipeline = make_pipeline(preprocessor, LogisticRegression(solver='saga',max_iter=10000,tol=0.0001,class_weight='balanced',penalty='l1',fit_intercept=False))\n",
    "\n",
    "# Create a GridSearchCV object to perform the search\n",
    "grid = GridSearchCV(pipeline, hyperparameters, cv=5, scoring='f1_macro',n_jobs =-1, verbose = 6)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fit the GridSearchCV object to the training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/data_analyse_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/data_analyse_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1389\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1388\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/miniconda3/envs/data_analyse_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/data_analyse_env/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/data_analyse_env/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/miniconda3/envs/data_analyse_env/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_analyse_env/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/envs/data_analyse_env/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the GridSearchCV object to the training data\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best hyperparameters:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)\n",
    "\n",
    "# Use the best hyperparameters to make predictions on the test data\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Print the classification report and confusion matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analyse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efb016019911489f5a03966f4d1632bb4674bdc6f12f4d95624e206dc03ec67a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
