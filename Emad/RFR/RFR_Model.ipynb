{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sweetviz as sv\n",
    "import missingno as msno\n",
    "import time \n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer, make_column_transformer\n",
    "# sklearn.compose: The sklearn.compose module is a submodule of the sklearn library for machine learning in Python. It provides functions for creating complex preprocessing and modeling pipelines.\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,PolynomialFeatures,RobustScaler\n",
    "#sklearn.preprocessing: The sklearn.preprocessing module is a submodule of the sklearn library that provides functions for preprocessing data, such as scaling and normalizing features, imputing missing values, and encoding categorical variables.\n",
    "from sklearn.linear_model import Ridge,LinearRegression,Lasso, ElasticNet\n",
    "# sklearn.linear_model: The sklearn.linear_model module is a submodule of the sklearn library that provides functions for fitting linear models for regression and classification.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# sklearn.pipeline: The sklearn.pipeline module is a submodule of the sklearn library that provides functions for creating and working with pipelines of transformers and models.\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,learning_curve, RandomizedSearchCV, cross_val_score, KFold\n",
    "# sklearn.model_selection: The sklearn.model_selection module is a submodule of the sklearn library that provides functions for splitting data into training and test sets, evaluating models using cross-validation, and hyperparameter tuning.\n",
    "from sklearn.dummy import DummyRegressor\n",
    "# sklearn.dummy: The sklearn.dummy module is a submodule of the sklearn library that provides simple dummy models for regression and classification.\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-f0f59de9cb82>:4: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('C:/Users/emada/Downloads/loan_project (1)/SBAnational.csv')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/home/apprenant/Documents/archive/SBAnational.csv')\n",
    "except:\n",
    "    df = pd.read_csv('C:/Users/emada/Downloads/loan_project (1)/SBAnational.csv')\n",
    "\n",
    "df['ApprovalFY'] = df['ApprovalFY'].replace('A', '', regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-248274118137>:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df[col] = df[col].str.replace(\"$\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# create a list of all numeric columns\n",
    "columns_to_transform_to_int = [\"DisbursementGross\",\"BalanceGross\",\"ChgOffPrinGr\",\"GrAppv\",\"SBA_Appv\"]\n",
    "\n",
    "\n",
    "for col in columns_to_transform_to_int:\n",
    "    df[col] = df[col].str.replace(\"$\", \"\")\n",
    "    df[col] = df[col].str.replace(\",\", \"\")\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "\n",
    "df['Term'] = df['Term'].astype(int)\n",
    "\n",
    "df['LowDoc'] = df['LowDoc'].replace({'0': 'N'})\n",
    "df = df[df['LowDoc'].isin(['N','Y'])]\n",
    "df['RevLineCr'] = df['RevLineCr'].replace({'0': 'N', 'T':'Y'})\n",
    "df = df[df['RevLineCr'].isin(['N','Y'])]\n",
    "\n",
    "dictionnaire =  {\"11\":\"Agriculture, forestry, fishing and hunting\",\"72\":\"Accommodation and food services\",\"21\":\"Mining, quarrying, and oil and gas extraction\",\"22\":\n",
    "\"Utilities\",\"23\":\"Construction\",\"31\":\"Manufacturing\",\"32\":\"Manufacturing\",\"33\":\"Manufacturing\",\"42\":\"Wholesale trade\",\"44\":\"Retail trade\",\"45\":\"Retail trade\",\"48\":\" Transportation and warehousing\", \"49\":\"Transportation and warehousing\", \"51\":\"Information\",\"52\":\"Finance and insurance\", \"53\":\"Real estate and rental and leasing\",\"54\":\"Professional, scientific, and technical services\",\"55\":\"Management of companies and enterprises\",\"56\":\"Administrative and support and waste management and remediation services\",\"61\":\"Educational services\",\"62\":\"Health care and social assistance\",\"71\":\"Arts, entertainment, and recreation\",\"81\":\"Other services (except public administration)\", \"92\": \"Public administration\", \"0\" : \"Other\"}\n",
    "\n",
    "df['NAICS'] = df['NAICS'].astype(str).str[:2]\n",
    "\n",
    "df[\"NAICS\"] = df[\"NAICS\"].map(dictionnaire)\n",
    "\n",
    "cols_to_drop = ['LoanNr_ChkDgt', 'Name', 'City', 'Zip', 'Bank', 'BankState', 'ApprovalDate', 'RetainedJob','ChgOffDate', 'DisbursementDate', 'DisbursementGross', 'BalanceGross', 'SBA_Appv', 'ChgOffPrinGr']\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "df_cleaned.dropna(subset=['MIS_Status'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleaned.MIS_Status\n",
    "X = df_cleaned.drop(columns=['MIS_Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline_to_ML(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.8, random_state=42)\n",
    "    numerical_features = make_column_selector(dtype_include=np.number)\n",
    "    categorical_features = make_column_selector(dtype_exclude= np.number)\n",
    "    numerical_pipeline = make_pipeline(SimpleImputer(strategy='mean'),StandardScaler(with_mean=False))\n",
    "    categorical_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder(handle_unknown='ignore'))\n",
    "    preprocessor = make_column_transformer((numerical_pipeline, numerical_features),\n",
    "                                    (categorical_pipeline, categorical_features)\n",
    "                                    )\n",
    "    return preprocessor, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "preprocessor, X_train, X_test, y_train, y_test = make_pipeline_to_ML(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest\n",
    "![forest](https://thumbs.gfycat.com/UncomfortableWelllitDrever-size_restricted.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler(with_mean=False))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x00000289AE4D4D90>),\n",
       "                                                 ('pipeline-2',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x00000289AE4D4A30>)])),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(class_weight='balanced', max_depth=10,\n",
       "                                        min_samples_leaf=700,\n",
       "                                        min_samples_split=4800,\n",
       "                                        n_estimators=200, n_jobs=-1,\n",
       "                                        random_state=42))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the RandomForestClassifier to the pipeline\n",
    "clf = make_pipeline(preprocessor, RandomForestClassifier(random_state=42, \n",
    "                                                        max_depth = 10, \n",
    "                                                        n_estimators = 200,\n",
    "                                                        min_samples_split = 4800,\n",
    "                                                        min_samples_leaf = 700,\n",
    "                                                        n_jobs=-1,\n",
    "                                                        class_weight='balanced'))\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param1': 'CA',\n",
       " 'param2': 'Other',\n",
       " 'param3': 1997,\n",
       " 'param4': 240,\n",
       " 'param5': 6,\n",
       " 'param6': 1,\n",
       " 'param7': 0,\n",
       " 'param8': 1,\n",
       " 'param9': 0,\n",
       " 'param10': 'Y',\n",
       " 'param11': 'N',\n",
       " 'param12': 540000}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"State\": \"CA\",\n",
    "  \"NAICS\": \"Other\",\n",
    "  \"ApprovalFY\": 1997,\n",
    "  \"Term\": 240,\n",
    "  \"NoEmp\": 6,\n",
    "  \"NewExist\": 1,\n",
    "  \"CreateJob\": 0,\n",
    "  \"FranchiseCode\": 1,\n",
    "  \"UrbanRural\": 0,\n",
    "  \"RevLineCr\": \"Y\",\n",
    "  \"LowDoc\": \"N\",\n",
    "  \"GrAppv\": 540000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model to a file\n",
    "with open(\"RFR_Model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "pickle_in = open('RFR_Model.pkl', 'rb') \n",
    "forest_model =pickle.load(pickle_in)\n",
    "\n",
    "def get_prediction(State, NAICS, ApprovalFY, Term, NoEmp, NewExist, CreateJob, FranchiseCode, UrbanRural, RevLineCr, LowDoc, GrAppv):\n",
    "    x = [[State, NAICS, ApprovalFY, Term, NoEmp, NewExist, CreateJob, FranchiseCode, UrbanRural, RevLineCr, LowDoc, GrAppv]]\n",
    "    df = pd.DataFrame(x, columns=['State', 'NAICS', 'ApprovalFY', 'Term', 'NoEmp', 'NewExist', 'CreateJob', 'FranchiseCode', 'UrbanRural', 'RevLineCr', 'LowDoc', 'GrAppv'])\n",
    "    y = forest_model.predict(df)[0]\n",
    "    prob = forest_model.predict_proba(df)[0].tolist()\n",
    "    return {'prediction': str(y), 'probability': prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P I F']\n"
     ]
    }
   ],
   "source": [
    "x_test = [['CA', 'Other', 1997, 240, 6, 1, 0, 1, 0, 'Y', 'N', 540000]]\n",
    "df_test = pd.DataFrame(x_test, columns=['State', 'NAICS', 'ApprovalFY', 'Term', 'NoEmp', 'NewExist', 'CreateJob', 'FranchiseCode', 'UrbanRural', 'RevLineCr', 'LowDoc', 'GrAppv'])\n",
    "\n",
    "# Make prediction using the clf model\n",
    "y_pred = forest_model.predict(df_test)\n",
    "\n",
    "# Print the prediction\n",
    "print(y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
